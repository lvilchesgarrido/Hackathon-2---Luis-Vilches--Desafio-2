{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08f6bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el notebook completo en un archivo .ipynb\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Cargar datos\n",
    "costos_df = pd.read_excel('Costos_Subprocesos.xlsx')\n",
    "indicadores_df = pd.read_excel('Indicadores_Perforación.xlsx')\n",
    "\n",
    "# Convertir las fechas a formato datetime\n",
    "costos_df['Subproceso_Costo'] = pd.to_datetime(costos_df['Subproceso_Costo'])\n",
    "indicadores_df['Mina/Rajo_Flota_Indicador'] = pd.to_datetime(indicadores_df['Mina/Rajo_Flota_Indicador'])\n",
    "\n",
    "# Unir los DataFrames por la columna de fechas\n",
    "df_unificado = pd.merge(costos_df, indicadores_df, left_on='Subproceso_Costo', right_on='Mina/Rajo_Flota_Indicador', how='inner')\n",
    "df_unificado = df_unificado.drop(columns=['Mina/Rajo_Flota_Indicador'])\n",
    "\n",
    "# Calcular la correlación entre todos los subprocesos y los indicadores\n",
    "subprocesos_cols = costos_df.columns[1:]  # Excluyendo la columna de fecha\n",
    "indicadores_cols = indicadores_df.columns[1:]  # Excluyendo la columna de fecha\n",
    "correlation_matrix = df_unificado.corr()\n",
    "correlation_filtered = correlation_matrix.loc[subprocesos_cols, indicadores_cols]\n",
    "top_correlations = correlation_filtered.apply(lambda x: x.nlargest(3), axis=1)\n",
    "\n",
    "# Dividir los datos en entrenamiento (2016-2021) y prueba (2022)\n",
    "train_data = df_unificado[df_unificado['Subproceso_Costo'] < '2022-01-01']\n",
    "test_data = df_unificado[df_unificado['Subproceso_Costo'] == '2022-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "068a426f-4bb9-4562-b55c-b01ba601ee7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para Linear Regression (Múltiple):\n",
      "                                        Predicción 2022  Costo Real 2022  \\\n",
      "ADM OPERACIÓN Y GESTIÓN ESP              1695594.857562              0.0   \n",
      "ADM OPERACIÓN Y GESTIÓN ESP SUR             8927.441737         30.27672   \n",
      "ADM OPERACIÓN Y GESTIÓN MIR               160701.379119              0.0   \n",
      "ADM OPERACIÓN Y GESTIÓN OXE              -464310.785967              0.0   \n",
      "ADM OPERACIÓN Y GESTIÓN TC                170370.255256        906.14448   \n",
      "ADM OPERACIÓN Y GESTIÓN TES SUR           116776.595251              0.0   \n",
      "ADMINISTRACIÓN DESARROLLO MINA              1155.486889     881286.83064   \n",
      "ADMINISTRACIÓN MINA                        -8620.083726    1780999.25816   \n",
      "CARGUÍO                                 -1267324.063292     833630.60568   \n",
      "CARGUÍO ENCUENTRO                           -15193.3529         262.9632   \n",
      "CARGUÍO ESPERANZA                        2416230.982762              0.0   \n",
      "CARGUÍO ESPERANZA SUR                       1592.648885              0.0   \n",
      "CARGUÍO MIRADOR                           397333.316555              0.0   \n",
      "CARGUÍO TESORO CENTRAL                    573414.545636     798743.62376   \n",
      "CARGUÍO TESORO SUR                        235734.205388              0.0   \n",
      "EQUIPOS  AUXILIARES                      -617243.184662     1148824.0217   \n",
      "EQUIPOS DE SOPORTE (APOYO)                -26709.481136     94203.674547   \n",
      "MANTENIMIENTO MINA                       -844511.046559    2753175.61454   \n",
      "MINA STRIPPING ESPERANZA                     -33.858209              0.0   \n",
      "PERFORACIÓN                              -379576.003837   1126857.545153   \n",
      "PERFORACIÓN ENCUENTRO                       4419.559467              0.0   \n",
      "PERFORACIÓN ESPERANZA                    1537080.300933     334431.59872   \n",
      "PERFORACIÓN ESPERANZA SUR                   2395.057739      55875.64824   \n",
      "PERFORACIÓN MIRADOR                        76408.710851     152843.68776   \n",
      "PERFORACIÓN TESORO CENTRAL                131489.229831     137664.98448   \n",
      "PERFORACIÓN TESORO NOR ESTE                   -3.344815              0.0   \n",
      "PERFORACIÓN TESORO SUR                     59133.967764       7634.64688   \n",
      "SERVICIOS DE APOYO MINA                  -409303.955266   3411129.441513   \n",
      "SERVICIOS TERCEROS ESP                    -13015.431585              0.0   \n",
      "SERVICIOS TERCEROS ESP SUR               5187741.024258   10981173.94088   \n",
      "SERVICIOS TERCEROS LLANO                            0.0              0.0   \n",
      "SERVICIOS TERCEROS MIR                       416.416787        316.51456   \n",
      "SERVICIOS TERCEROS OXE                   7227169.265394   14936153.07776   \n",
      "SERVICIOS TERCEROS TC                      -1249.749006              0.0   \n",
      "SERVICIOS Y EQUIPOS DE APOYO ENCUENTRO     -3622.084166              0.0   \n",
      "SERVICIOS Y EQUIPOS DE APOYO ESP SUR        2611.895198      15110.39448   \n",
      "SERVICIOS Y EQUIPOS DE APOYO ESPERANZA   2912666.355958         166.3636   \n",
      "SERVICIOS Y EQUIPOS DE APOYO MIRADOR      465578.785456              0.0   \n",
      "SERVICIOS Y EQUIPOS DE APOYO TES SUR       220502.27762              0.0   \n",
      "SERVICIOS Y EQUIPOS DE APOYO TESORO C.    679765.617342              0.0   \n",
      "TRANSPORTE                              -2638847.899358  17323908.505433   \n",
      "TRANSPORTE ENCUENTRO                      -34610.247017              0.0   \n",
      "TRANSPORTE ESPERANZA                    11135706.174991              0.0   \n",
      "TRANSPORTE ESPERANZA SUR                    2177.378868              0.0   \n",
      "TRANSPORTE MIRADOR                         525884.62081              0.0   \n",
      "TRANSPORTE TESORO CENTRAL                1339805.597869              0.0   \n",
      "TRANSPORTE TESORO SUR                     491954.856152              0.0   \n",
      "TRONADURA ENCUENTRO                       764160.843226    1073451.66592   \n",
      "TRONADURA ESPERANZA                       1805589.74558    2435569.77224   \n",
      "TRONADURA ESPERANZA SUR                    51086.248789      1231605.096   \n",
      "TRONADURA MIRADOR                         103318.360155       341437.856   \n",
      "TRONADURA TESORO CENTRAL                  296106.413731       306093.616   \n",
      "TRONADURA TESORO NOR ESTE                           0.0              0.0   \n",
      "TRONADURA TESORO SUR                       97432.244985        48472.712   \n",
      "\n",
      "                                             Diferencia    Diferencia %  \n",
      "ADM OPERACIÓN Y GESTIÓN ESP              1695594.857562             N/A  \n",
      "ADM OPERACIÓN Y GESTIÓN ESP SUR             8897.165017    29386.158796  \n",
      "ADM OPERACIÓN Y GESTIÓN MIR               160701.379119             N/A  \n",
      "ADM OPERACIÓN Y GESTIÓN OXE              -464310.785967             N/A  \n",
      "ADM OPERACIÓN Y GESTIÓN TC                169464.110776     18701.66563  \n",
      "ADM OPERACIÓN Y GESTIÓN TES SUR           116776.595251             N/A  \n",
      "ADMINISTRACIÓN DESARROLLO MINA           -880131.343751      -99.868886  \n",
      "ADMINISTRACIÓN MINA                     -1789619.341886     -100.484003  \n",
      "CARGUÍO                                 -2100954.668972     -252.024656  \n",
      "CARGUÍO ENCUENTRO                           -15456.3161    -5877.748712  \n",
      "CARGUÍO ESPERANZA                        2416230.982762             N/A  \n",
      "CARGUÍO ESPERANZA SUR                       1592.648885             N/A  \n",
      "CARGUÍO MIRADOR                           397333.316555             N/A  \n",
      "CARGUÍO TESORO CENTRAL                   -225329.078124      -28.210438  \n",
      "CARGUÍO TESORO SUR                        235734.205388             N/A  \n",
      "EQUIPOS  AUXILIARES                     -1766067.206362     -153.728262  \n",
      "EQUIPOS DE SOPORTE (APOYO)               -120913.155683     -128.352908  \n",
      "MANTENIMIENTO MINA                      -3597686.661099     -130.674071  \n",
      "MINA STRIPPING ESPERANZA                     -33.858209             N/A  \n",
      "PERFORACIÓN                              -1506433.54899     -133.684471  \n",
      "PERFORACIÓN ENCUENTRO                       4419.559467             N/A  \n",
      "PERFORACIÓN ESPERANZA                    1202648.702213       359.60977  \n",
      "PERFORACIÓN ESPERANZA SUR                 -53480.590501      -95.713593  \n",
      "PERFORACIÓN MIRADOR                       -76434.976909      -50.008592  \n",
      "PERFORACIÓN TESORO CENTRAL                 -6175.754649       -4.486075  \n",
      "PERFORACIÓN TESORO NOR ESTE                   -3.344815             N/A  \n",
      "PERFORACIÓN TESORO SUR                     51499.320884      674.547516  \n",
      "SERVICIOS DE APOYO MINA                 -3820433.396779     -111.999074  \n",
      "SERVICIOS TERCEROS ESP                    -13015.431585             N/A  \n",
      "SERVICIOS TERCEROS ESP SUR              -5793432.916622      -52.757865  \n",
      "SERVICIOS TERCEROS LLANO                            0.0             N/A  \n",
      "SERVICIOS TERCEROS MIR                        99.902227       31.563233  \n",
      "SERVICIOS TERCEROS OXE                  -7708983.812366      -51.612914  \n",
      "SERVICIOS TERCEROS TC                      -1249.749006             N/A  \n",
      "SERVICIOS Y EQUIPOS DE APOYO ENCUENTRO     -3622.084166             N/A  \n",
      "SERVICIOS Y EQUIPOS DE APOYO ESP SUR      -12498.499282       -82.71458  \n",
      "SERVICIOS Y EQUIPOS DE APOYO ESPERANZA   2912499.992358  1750683.438179  \n",
      "SERVICIOS Y EQUIPOS DE APOYO MIRADOR      465578.785456             N/A  \n",
      "SERVICIOS Y EQUIPOS DE APOYO TES SUR       220502.27762             N/A  \n",
      "SERVICIOS Y EQUIPOS DE APOYO TESORO C.    679765.617342             N/A  \n",
      "TRANSPORTE                             -19962756.404791     -115.232405  \n",
      "TRANSPORTE ENCUENTRO                      -34610.247017             N/A  \n",
      "TRANSPORTE ESPERANZA                    11135706.174991             N/A  \n",
      "TRANSPORTE ESPERANZA SUR                    2177.378868             N/A  \n",
      "TRANSPORTE MIRADOR                         525884.62081             N/A  \n",
      "TRANSPORTE TESORO CENTRAL                1339805.597869             N/A  \n",
      "TRANSPORTE TESORO SUR                     491954.856152             N/A  \n",
      "TRONADURA ENCUENTRO                      -309290.822694      -28.812739  \n",
      "TRONADURA ESPERANZA                       -629980.02666      -25.865817  \n",
      "TRONADURA ESPERANZA SUR                 -1180518.847211      -95.852059  \n",
      "TRONADURA MIRADOR                        -238119.495845      -69.740215  \n",
      "TRONADURA TESORO CENTRAL                   -9987.202269       -3.262793  \n",
      "TRONADURA TESORO NOR ESTE                           0.0             N/A  \n",
      "TRONADURA TESORO SUR                       48959.532985       101.00432  \n"
     ]
    }
   ],
   "source": [
    "# Modelo 1: Linear Regression (Múltiple)\n",
    "resultados_lr = {}\n",
    "lr_model = LinearRegression()\n",
    "for subproceso in subprocesos_cols:\n",
    "    indicadores_seleccionados = top_correlations.loc[subproceso].dropna().index.tolist()\n",
    "    if len(indicadores_seleccionados) > 0:\n",
    "        X_train = train_data[indicadores_seleccionados]\n",
    "        y_train = train_data[subproceso]\n",
    "        X_test = test_data[indicadores_seleccionados]\n",
    "        \n",
    "        # Eliminar filas con NaN en X_train y y_train\n",
    "        X_y_train = pd.concat([X_train, y_train], axis=1).dropna()\n",
    "        X_train = X_y_train.iloc[:, :-1].values\n",
    "        y_train = X_y_train.iloc[:, -1].values\n",
    "        \n",
    "        # Eliminar filas con NaN en X_test\n",
    "        X_test = X_test.dropna().values\n",
    "\n",
    "        # Entrenar el modelo\n",
    "        lr_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Realizar la predicción para el 2022\n",
    "        y_pred_2022 = lr_model.predict(X_test)[0]\n",
    "        costo_real_2022 = costos_df[costos_df['Subproceso_Costo'] == '2022-01-01'][subproceso].values[0]\n",
    "        diferencia = y_pred_2022 - costo_real_2022\n",
    "        diferencia_pct = (diferencia / costo_real_2022) * 100 if costo_real_2022 != 0 else 'N/A'\n",
    "        resultados_lr[subproceso] = {'Predicción 2022': y_pred_2022, 'Costo Real 2022': costo_real_2022, 'Diferencia': diferencia, 'Diferencia %': diferencia_pct}\n",
    "\n",
    "resultados_lr_df = pd.DataFrame(resultados_lr).T\n",
    "print(\"Resultados para Linear Regression (Múltiple):\")\n",
    "print(resultados_lr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf18920-2198-4754-a567-e84b2d19c142",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nPolynomialFeatures does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m X_test \u001b[38;5;241m=\u001b[39m test_data[indicadores_seleccionados]\n\u001b[0;32m     10\u001b[0m poly \u001b[38;5;241m=\u001b[39m PolynomialFeatures(degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m X_train_poly \u001b[38;5;241m=\u001b[39m \u001b[43mpoly\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m X_test_poly \u001b[38;5;241m=\u001b[39m poly\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m     14\u001b[0m pr_model \u001b[38;5;241m=\u001b[39m LinearRegression()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_polynomial.py:323\u001b[0m, in \u001b[0;36mPolynomialFeatures.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    307\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m    Compute number of output features.\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m        Fitted transformer.\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 323\u001b[0m     _, n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegree, Integral):\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegree \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_bias:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nPolynomialFeatures does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Modelo 2: Polynomial Regression (Regresión Polinómica)\n",
    "resultados_pr = {}\n",
    "for subproceso in subprocesos_cols:\n",
    "    indicadores_seleccionados = top_correlations.loc[subproceso].dropna().index.tolist()\n",
    "    if len(indicadores_seleccionados) > 0:\n",
    "        X_train = train_data[indicadores_seleccionados]\n",
    "        y_train = train_data[subproceso]\n",
    "        X_test = test_data[indicadores_seleccionados]\n",
    "        \n",
    "        poly = PolynomialFeatures(degree=2)\n",
    "        X_train_poly = poly.fit_transform(X_train)\n",
    "        X_test_poly = poly.transform(X_test)\n",
    "        \n",
    "        pr_model = LinearRegression()\n",
    "        pr_model.fit(X_train_poly, y_train)\n",
    "        y_pred_2022 = pr_model.predict(X_test_poly)[0]\n",
    "        costo_real_2022 = costos_df[costos_df['Subproceso_Costo'] == '2022-01-01'][subproceso].values[0]\n",
    "        diferencia = y_pred_2022 - costo_real_2022\n",
    "        diferencia_pct = (diferencia / costo_real_2022) * 100 if costo_real_2022 != 0 else 'N/A'\n",
    "        resultados_pr[subproceso] = {'Predicción 2022': y_pred_2022, 'Costo Real 2022': costo_real_2022, 'Diferencia': diferencia, 'Diferencia %': diferencia_pct}\n",
    "resultados_pr_df = pd.DataFrame(resultados_pr).T\n",
    "print(\"Resultados para Polynomial Regression (Regresión Polinómica):\")\n",
    "print(resultados_pr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d847edf-7372-49c5-9b30-3051e549a67c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Modelo 3: Neural Network (Redes Neuronales)\n",
    "resultados_nn = {}\n",
    "nn_model = MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
    "for subproceso in subprocesos_cols:\n",
    "    indicadores_seleccionados = top_correlations.loc[subproceso].dropna().index.tolist()\n",
    "    if len(indicadores_seleccionados) > 0:\n",
    "        X_train = train_data[indicadores_seleccionados]\n",
    "        y_train = train_data[subproceso]\n",
    "        X_test = test_data[indicadores_seleccionados]\n",
    "        nn_model.fit(X_train, y_train)\n",
    "        y_pred_2022 = nn_model.predict(X_test)[0]\n",
    "        costo_real_2022 = costos_df[costos_df['Subproceso_Costo'] == '2022-01-01'][subproceso].values[0]\n",
    "        diferencia = y_pred_2022 - costo_real_2022\n",
    "        diferencia_pct = (diferencia / costo_real_2022) * 100 if costo_real_2022 != 0 else 'N/A'\n",
    "        resultados_nn[subproceso] = {'Predicción 2022': y_pred_2022, 'Costo Real 2022': costo_real_2022, 'Diferencia': diferencia, 'Diferencia %': diferencia_pct}\n",
    "resultados_nn_df = pd.DataFrame(resultados_nn).T\n",
    "print(\"Resultados para Neural Network (Redes Neuronales):\")\n",
    "print(resultados_nn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b943cada-55a2-4e41-8fd6-11e06f2779bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 4: Random Forest\n",
    "resultados_rf = {}\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "for subproceso in subprocesos_cols:\n",
    "    indicadores_seleccionados = top_correlations.loc[subproceso].dropna().index.tolist()\n",
    "    if len(indicadores_seleccionados) > 0:\n",
    "        X_train = train_data[indicadores_seleccionados]\n",
    "        y_train = train_data[subproceso]\n",
    "        X_test = test_data[indicadores_seleccionados]\n",
    "        rf_model.fit(X_train, y_train)\n",
    "        y_pred_2022 = rf_model.predict(X_test)[0]\n",
    "        costo_real_2022 = costos_df[costos_df['Subproceso_Costo'] == '2022-01-01'][subproceso].values[0]\n",
    "        diferencia = y_pred_2022 - costo_real_2022\n",
    "        diferencia_pct = (diferencia / costo_real_2022) * 100 if costo_real_2022 != 0 else 'N/A'\n",
    "        resultados_rf[subproceso] = {'Predicción 2022': y_pred_2022, 'Costo Real 2022': costo_real_2022, 'Diferencia': diferencia, 'Diferencia %': diferencia_pct}\n",
    "resultados_rf_df = pd.DataFrame(resultados_rf).T\n",
    "print(\"Resultados para Random Forest:\")\n",
    "print(resultados_rf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5f2330-bef7-4605-b644-0b453683b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 5: Gradient Boosting\n",
    "resultados_gb = {}\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "for subproceso in subprocesos_cols:\n",
    "    indicadores_seleccionados = top_correlations.loc[subproceso].dropna().index.tolist()\n",
    "    if len(indicadores_seleccionados) > 0:\n",
    "        X_train = train_data[indicadores_seleccionados]\n",
    "        y_train = train_data[subproceso]\n",
    "        X_test = test_data[indicadores_seleccionados]\n",
    "        gb_model.fit(X_train, y_train)\n",
    "        y_pred_2022 = gb_model.predict(X_test)[0]\n",
    "        costo_real_2022 = costos_df[costos_df['Subproceso_Costo'] == '2022-01-01'][subproceso].values[0]\n",
    "        diferencia = y_pred_2022 - costo_real_2022\n",
    "        diferencia_pct = (diferencia / costo_real_2022) * 100 if costo_real_2022 != 0 else 'N/A'\n",
    "        resultados_gb[subproceso] = {'Predicción 2022': y_pred_2022, 'Costo Real 2022': costo_real_2022, 'Diferencia': diferencia, 'Diferencia %': diferencia_pct}\n",
    "resultados_gb_df = pd.DataFrame(resultados_gb).T\n",
    "print(\"Resultados para Gradient Boosting:\")\n",
    "print(resultados_gb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45936dba-e7c6-4b33-91c3-954a42294616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 6: XGBoost\n",
    "resultados_xgb = {}\n",
    "xgb_model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "for subproceso in subprocesos_cols:\n",
    "    indicadores_seleccionados = top_correlations.loc[subproceso].dropna().index.tolist()\n",
    "    if len(indicadores_seleccionados) > 0:\n",
    "        X_train = train_data[indicadores_seleccionados]\n",
    "        y_train = train_data[subproceso]\n",
    "        X_test = test_data[indicadores_seleccionados]\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred_2022 = xgb_model.predict(X_test)[0]\n",
    "        costo_real_2022 = costos_df[costos_df['Subproceso_Costo'] == '2022-01-01'][subproceso].values[0]\n",
    "        diferencia = y_pred_2022 - costo_real_2022\n",
    "        diferencia_pct = (diferencia / costo_real_2022) * 100 if costo_real_2022 != 0 else 'N/A'\n",
    "        resultados_xgb[subproceso] = {'Predicción 2022': y_pred_2022, 'Costo Real 2022': costo_real_2022, 'Diferencia': diferencia, 'Diferencia %': diferencia_pct}\n",
    "resultados_xgb_df = pd.DataFrame(resultados_xgb).T\n",
    "print(\"Resultados para XGBoost:\")\n",
    "print(resultados_xgb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78abedf-bf63-4904-841b-eedc06e53935",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comparación de resultados\n",
    "resultados_totales = {\n",
    "    'Linear Regression (Múltiple)': resultados_lr_df,\n",
    "    'Polynomial Regression': resultados_pr_df,\n",
    "    'Neural Network': resultados_nn_df,\n",
    "    'Random Forest': resultados_rf_df,\n",
    "    'Gradient Boosting': resultados_gb_df,\n",
    "    'XGBoost': resultados_xgb_df\n",
    "}\n",
    "for modelo, resultados in resultados_totales.items():\n",
    "    print(f\"Resumen para {modelo}:\")\n",
    "    print(resultados[['Diferencia %']])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118168df-09bb-4be5-a058-a74b5ad2ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Resultados de los modelos (simula los DataFrames que imprimiste anteriormente)\n",
    "resultados = {\n",
    "    'Linear Regression': resultados_lr_df,\n",
    "    'Polynomial Regression': resultados_pr_df,\n",
    "    'Neural Network': resultados_nn_df,\n",
    "    'Random Forest': resultados_rf_df,\n",
    "    'Gradient Boosting': resultados_gb_df,\n",
    "    'XGBoost': resultados_xgb_df\n",
    "}\n",
    "\n",
    "# Crear un DataFrame para almacenar los mejores modelos para cada subproceso\n",
    "mejor_modelo_df = pd.DataFrame(columns=['Subproceso', 'Mejor Modelo', 'Diferencia %'])\n",
    "\n",
    "# Comparar los resultados para cada subproceso\n",
    "for subproceso in resultados['Linear Regression'].index:\n",
    "    mejor_modelo = None\n",
    "    menor_diferencia = np.inf\n",
    "    \n",
    "    for modelo, df in resultados.items():\n",
    "        if subproceso in df.index and df.loc[subproceso, 'Diferencia %'] != 'N/A':\n",
    "            diferencia = abs(df.loc[subproceso, 'Diferencia %'])\n",
    "            if diferencia < menor_diferencia:\n",
    "                menor_diferencia = diferencia\n",
    "                mejor_modelo = modelo\n",
    "    \n",
    "    # Guardar el mejor modelo para el subproceso\n",
    "    nueva_fila = pd.DataFrame({\n",
    "        'Subproceso': [subproceso],\n",
    "        'Mejor Modelo': [mejor_modelo],\n",
    "        'Diferencia %': [menor_diferencia]\n",
    "    })\n",
    "    \n",
    "    mejor_modelo_df = pd.concat([mejor_modelo_df, nueva_fila], ignore_index=True)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(mejor_modelo_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
